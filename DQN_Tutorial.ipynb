{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Tutorial",
      "provenance": [],
      "collapsed_sections": [
        "Oj6aTY1AIKVg",
        "OIb4SMcWskTb",
        "eVIEfsDg98F4",
        "n98Ll4VZKcPB",
        "Y8ftjos6Kqod"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathilde-schveitzer/signal-generator/blob/main/DQN_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vbrBxWc8O2"
      },
      "source": [
        "# Licence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTl-ivwIcS38"
      },
      "source": [
        "\n",
        "#Licence\n",
        "#Copyright 2021 Google LLC.\n",
        "#SPDX-License-Identifier: Apache-2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErkTGyQ0Qase"
      },
      "source": [
        "#DQN Tutorial\n",
        "This tutorial is an adaptation of the much deeper and wider tutorial that can be found <a href=\"https://github.com/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb\">here</a>.\n",
        "\n",
        "During this tutorial, we will try to reimplement, partly from scratch, a simple version of <a href=\"https://arxiv.org/abs/1312.5602\">Deep Q-learning</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVaNs4WEQekY"
      },
      "source": [
        "##Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKP2o0QuQhpd"
      },
      "source": [
        "### Install required libraries\n",
        "\n",
        "1. [Acme](https://github.com/deepmind/acme) is a library of reinforcement learning (RL) agents and agent building blocks. Acme strives to expose simple, efficient, and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research. The design of Acme also attempts to provide multiple points of entry to the RL problem at differing levels of complexity.\n",
        "\n",
        "\n",
        "2. [Haiku](https://github.com/deepmind/dm-haiku) is a simple neural network library for JAX developed by some of the authors of Sonnet, a neural network library for TensorFlow.\n",
        "\n",
        "3. [dm_env](https://github.com/deepmind/dm_env): DeepMind Environment API, which will be covered in more details in the [Environment subsection](https://colab.research.google.com/drive/1oKyyhOFAFSBTpVnmuOm9HXh5D5ekqhh5#scrollTo=I6KuVGSk4uc9) below.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gDZSbrcjQkJ5"
      },
      "source": [
        "#@title Installations  { form-width: \"30%\" }\n",
        "\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb]\n",
        "!pip install dm-acme[jax]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-acme[envs]\n",
        "!pip install dm-env\n",
        "!pip install dm-haiku\n",
        "!pip install chex\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install imageio\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBqd3jWPQ6YJ"
      },
      "source": [
        "#@title Imports  { form-width: \"30%\" }\n",
        "\n",
        "import IPython\n",
        "\n",
        "import acme\n",
        "from acme import environment_loop\n",
        "from acme import datasets\n",
        "from acme import types\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.wrappers import gym_wrapper\n",
        "from acme.agents.jax import dqn\n",
        "from acme.adders import reverb as adders\n",
        "from acme.utils import counting\n",
        "from acme.utils import loggers\n",
        "import base64\n",
        "import chex\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import dm_env\n",
        "import enum\n",
        "import functools\n",
        "import gym\n",
        "import haiku as hk\n",
        "import io\n",
        "import itertools\n",
        "import jax\n",
        "from jax import tree_util\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import reverb\n",
        "import rlax\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6WHNe9WQ8-g"
      },
      "source": [
        "# 1. Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmhfHAMRRBnJ"
      },
      "source": [
        "We will focus on a simple grid world environment for this practical session. \n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1qBjh_PRdZ4GWTDqB9pmjLEOlUAsOfrZi\" width=\"500\" />\n",
        "\n",
        "\n",
        "\n",
        "This environment consists of either walls and empty cells. The agent starts from an initial location and needs to navigate to reach a goal location. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLJLM__lqQZ",
        "cellView": "form"
      },
      "source": [
        "#@title Gridworld Implementation { form-width: \"30%\" }\n",
        "\n",
        "\n",
        "class ObservationType(enum.IntEnum):\n",
        "  STATE_INDEX = enum.auto()\n",
        "  AGENT_ONEHOT = enum.auto()\n",
        "  GRID = enum.auto()\n",
        "  AGENT_GOAL_POS = enum.auto()\n",
        "\n",
        "\n",
        "class GridWorld(dm_env.Environment):\n",
        "\n",
        "  def __init__(self,\n",
        "               layout,\n",
        "               start_state,\n",
        "               goal_state=None,\n",
        "               observation_type=ObservationType.STATE_INDEX,\n",
        "               discount=0.9,\n",
        "               penalty_for_walls=-5,\n",
        "               reward_goal=10,\n",
        "               max_episode_length=None,\n",
        "               randomize_goals=False):\n",
        "    \"\"\"Build a grid environment.\n",
        "\n",
        "    Simple gridworld defined by a map layout, a start and a goal state.\n",
        "\n",
        "    Layout should be a NxN grid, containing:\n",
        "      * 0: empty\n",
        "      * -1: wall\n",
        "      * Any other positive value: value indicates reward; episode will terminate\n",
        "\n",
        "    Args:\n",
        "      layout: NxN array of numbers, indicating the layout of the environment.\n",
        "      start_state: Tuple (y, x) of starting location.\n",
        "      goal_state: Optional tuple (y, x) of goal location. Will be randomly\n",
        "        sampled once if None.\n",
        "      observation_type: Enum observation type to use. One of:\n",
        "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\n",
        "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the \n",
        "          agent is and 0 elsewhere.\n",
        "        * ObservationType.GRID: NxNx3 float32 grid of feature channels. \n",
        "          First channel contains walls (1 if wall, 0 otherwise), second the \n",
        "          agent position (1 if agent, 0 otherwise) and third goal position\n",
        "          (1 if goal, 0 otherwise)\n",
        "        * ObservationType.AGENT_GOAL_POS: float32 tuple with \n",
        "          (agent_y, agent_x, goal_y, goal_x)\n",
        "      discount: Discounting factor included in all Timesteps.\n",
        "      penalty_for_walls: Reward added when hitting a wall (should be negative).\n",
        "      reward_goal: Reward added when finding the goal (should be positive).\n",
        "      max_episode_length: If set, will terminate an episode after this many \n",
        "        steps.\n",
        "      randomize_goals: If true, randomize goal at every episode.\n",
        "    \"\"\"\n",
        "    if observation_type not in ObservationType:\n",
        "      raise ValueError('observation_type should be a ObservationType instace.')\n",
        "    self._layout = np.array(layout)\n",
        "    self._start_state = start_state\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._discount = discount\n",
        "    self._penalty_for_walls = penalty_for_walls\n",
        "    self._reward_goal = reward_goal\n",
        "    self._observation_type = observation_type\n",
        "    self._layout_dims = self._layout.shape\n",
        "    self._max_episode_length = max_episode_length\n",
        "    self._num_episode_steps = 0\n",
        "    self._randomize_goals = randomize_goals\n",
        "    if goal_state is None:\n",
        "      # Randomly sample goal_state if not provided\n",
        "      goal_state = self._sample_goal()\n",
        "    self.goal_state = goal_state\n",
        "\n",
        "  def _sample_goal(self):\n",
        "    \"\"\"Randomly sample reachable non-starting state.\"\"\"\n",
        "    # Sample a new goal\n",
        "    n = 0\n",
        "    max_tries = 1e5\n",
        "    while n < max_tries:\n",
        "      goal_state = tuple(np.random.randint(d) for d in self._layout_dims)\n",
        "      if goal_state != self._state and self._layout[goal_state] == 0:\n",
        "        # Reachable state found!\n",
        "        return goal_state\n",
        "      n += 1\n",
        "    raise ValueError('Failed to sample a goal state.')\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "    return self._number_of_states\n",
        "\n",
        "  @property\n",
        "  def goal_state(self):\n",
        "    return self._goal_state\n",
        "\n",
        "  def set_state(self, x, y):\n",
        "    self._state = (y, x)\n",
        "\n",
        "  @goal_state.setter\n",
        "  def goal_state(self, new_goal):\n",
        "    if new_goal == self._state or self._layout[new_goal] < 0:\n",
        "      raise ValueError('This is not a valid goal!')\n",
        "    # Zero out any other goal\n",
        "    self._layout[self._layout > 0] = 0\n",
        "    # Setup new goal location\n",
        "    self._layout[new_goal] = self._reward_goal\n",
        "    self._goal_state = new_goal\n",
        "\n",
        "  def observation_spec(self):\n",
        "    if self._observation_type is ObservationType.AGENT_ONEHOT:\n",
        "      return specs.Array(\n",
        "          shape=self._layout_dims,\n",
        "          dtype=np.float32,\n",
        "          name='observation_agent_onehot')\n",
        "    elif self._observation_type is ObservationType.GRID:\n",
        "      return specs.Array(\n",
        "          shape=self._layout_dims + (3,),\n",
        "          dtype=np.float32,\n",
        "          name='observation_grid')\n",
        "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\n",
        "      return specs.Array(\n",
        "          shape=(4,), dtype=np.float32, name='observation_agent_goal_pos')\n",
        "    elif self._observation_type is ObservationType.STATE_INDEX:\n",
        "      return specs.DiscreteArray(\n",
        "          self._number_of_states, dtype=int, name='observation_state_index')\n",
        "\n",
        "  def action_spec(self):\n",
        "    return specs.DiscreteArray(4, dtype=int, name='action')\n",
        "\n",
        "  def get_obs(self):\n",
        "    if self._observation_type is ObservationType.AGENT_ONEHOT:\n",
        "      obs = np.zeros(self._layout.shape, dtype=np.float32)\n",
        "      # Place agent\n",
        "      obs[self._state] = 1\n",
        "      return obs\n",
        "    elif self._observation_type is ObservationType.GRID:\n",
        "      obs = np.zeros(self._layout.shape + (3,), dtype=np.float32)\n",
        "      obs[..., 0] = self._layout < 0\n",
        "      obs[self._state[0], self._state[1], 1] = 1\n",
        "      obs[self._goal_state[0], self._goal_state[1], 2] = 1\n",
        "      return obs\n",
        "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\n",
        "      return np.array(self._state + self._goal_state, dtype=np.float32)\n",
        "    elif self._observation_type is ObservationType.STATE_INDEX:\n",
        "      y, x = self._state\n",
        "      return y * self._layout.shape[1] + x\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._start_state\n",
        "    self._num_episode_steps = 0\n",
        "    if self._randomize_goals:\n",
        "      self.goal_state = self._sample_goal()\n",
        "    return dm_env.TimeStep(\n",
        "        step_type=dm_env.StepType.FIRST,\n",
        "        reward=None,\n",
        "        discount=None,\n",
        "        observation=self.get_obs())\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'Invalid action: {} is not 0, 1, 2, or 3.'.format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    step_type = dm_env.StepType.MID\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = self._penalty_for_walls\n",
        "      discount = self._discount\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = self._discount\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "      step_type = dm_env.StepType.LAST\n",
        "\n",
        "    self._state = new_state\n",
        "    self._num_episode_steps += 1\n",
        "    if (self._max_episode_length is not None and\n",
        "        self._num_episode_steps >= self._max_episode_length):\n",
        "      step_type = dm_env.StepType.LAST\n",
        "    return dm_env.TimeStep(\n",
        "        step_type=step_type,\n",
        "        reward=np.float32(reward),\n",
        "        discount=discount,\n",
        "        observation=self.get_obs())\n",
        "\n",
        "  def plot_grid(self, add_start=True):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout <= -1, interpolation='nearest')\n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    # Add start/goal\n",
        "    if add_start:\n",
        "      plt.text(\n",
        "          self._start_state[1],\n",
        "          self._start_state[0],\n",
        "          r'$\\mathbf{S}$',\n",
        "          fontsize=16,\n",
        "          ha='center',\n",
        "          va='center')\n",
        "    plt.text(\n",
        "        self._goal_state[1],\n",
        "        self._goal_state[0],\n",
        "        r'$\\mathbf{G}$',\n",
        "        fontsize=16,\n",
        "        ha='center',\n",
        "        va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h - 1):\n",
        "      plt.plot([-0.5, w - 0.5], [y + 0.5, y + 0.5], '-k', lw=2)\n",
        "    for x in range(w - 1):\n",
        "      plt.plot([x + 0.5, x + 0.5], [-0.5, h - 0.5], '-k', lw=2)\n",
        "\n",
        "  def plot_state(self, return_rgb=False):\n",
        "    self.plot_grid(add_start=False)\n",
        "    # Add the agent location\n",
        "    plt.text(\n",
        "        self._state[1],\n",
        "        self._state[0],\n",
        "        u'😃',\n",
        "        fontname='symbola',\n",
        "        fontsize=18,\n",
        "        ha='center',\n",
        "        va='center',\n",
        "    )\n",
        "    if return_rgb:\n",
        "      fig = plt.gcf()\n",
        "      plt.axis('tight')\n",
        "      plt.subplots_adjust(0, 0, 1, 1, 0, 0)\n",
        "      fig.canvas.draw()\n",
        "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "      w, h = fig.canvas.get_width_height()\n",
        "      data = data.reshape((h, w, 3))\n",
        "      plt.close(fig)\n",
        "      return data\n",
        "\n",
        "  def plot_policy(self, policy):\n",
        "    action_names = [\n",
        "        r'$\\uparrow$', r'$\\rightarrow$', r'$\\downarrow$', r'$\\leftarrow$'\n",
        "    ]\n",
        "    self.plot_grid()\n",
        "    plt.title('Policy Visualization')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h):\n",
        "      for x in range(w):\n",
        "        # if ((y, x) != self._start_state) and ((y, x) != self._goal_state):\n",
        "        if (y, x) != self._goal_state:\n",
        "          action_name = action_names[policy[y, x]]\n",
        "          plt.text(x, y, action_name, ha='center', va='center')\n",
        "\n",
        "  def plot_greedy_policy(self, q):\n",
        "    greedy_actions = np.argmax(q, axis=2)\n",
        "    self.plot_policy(greedy_actions)\n",
        "\n",
        "\n",
        "def build_gridworld_task(task,\n",
        "                         discount=0.9,\n",
        "                         penalty_for_walls=-5,\n",
        "                         observation_type=ObservationType.STATE_INDEX,\n",
        "                         max_episode_length=200):\n",
        "  \"\"\"Construct a particular Gridworld layout with start/goal states.\n",
        "\n",
        "  Args:\n",
        "      task: string name of the task to use. One of {'simple', 'obstacle', \n",
        "        'random_goal'}.\n",
        "      discount: Discounting factor included in all Timesteps.\n",
        "      penalty_for_walls: Reward added when hitting a wall (should be negative).\n",
        "      observation_type: Enum observation type to use. One of:\n",
        "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\n",
        "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the \n",
        "          agent is and 0 elsewhere.\n",
        "        * ObservationType.GRID: NxNx3 float32 grid of feature channels. \n",
        "          First channel contains walls (1 if wall, 0 otherwise), second the \n",
        "          agent position (1 if agent, 0 otherwise) and third goal position\n",
        "          (1 if goal, 0 otherwise)\n",
        "        * ObservationType.AGENT_GOAL_POS: float32 tuple with \n",
        "          (agent_y, agent_x, goal_y, goal_x).\n",
        "      max_episode_length: If set, will terminate an episode after this many \n",
        "        steps.\n",
        "  \"\"\"\n",
        "  tasks_specifications = {\n",
        "      'simple': {\n",
        "          'layout': [\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "          ],\n",
        "          'start_state': (2, 2),\n",
        "          'goal_state': (7, 2)\n",
        "      },\n",
        "      'obstacle': {\n",
        "          'layout': [\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, -1, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "          ],\n",
        "          'start_state': (2, 2),\n",
        "          'goal_state': (2, 8)\n",
        "      },\n",
        "      'random_goal': {\n",
        "          'layout': [\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "          ],\n",
        "          'start_state': (2, 2),\n",
        "          # 'randomize_goals': True\n",
        "      },\n",
        "  }\n",
        "  return GridWorld(\n",
        "      discount=discount,\n",
        "      penalty_for_walls=penalty_for_walls,\n",
        "      observation_type=observation_type,\n",
        "      max_episode_length=max_episode_length,\n",
        "      **tasks_specifications[task])\n",
        "\n",
        "\n",
        "def setup_environment(environment):\n",
        "  # Make sure the environment outputs single-precision floats.\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "\n",
        "  # Grab the spec of the environment.\n",
        "  environment_spec = specs.make_environment_spec(environment)\n",
        "\n",
        "  return environment, environment_spec"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXsgYxiVlwaJ"
      },
      "source": [
        "\n",
        "We will use two distinct tabular GridWorlds:\n",
        "* `simple` where the goal is at the bottom left of the grid, little navigation required.\n",
        "* `obstacle` where the goal is behind an obstacle to avoid.\n",
        "\n",
        "You can visualize the grid worlds by running the cell below. \n",
        "\n",
        "Note that `S` indicates the start state and `G` indicates the goal. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNha9NFul2ow",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "cellView": "form",
        "outputId": "a47b0d13-4960-4cbc-bfd3-6b391e8d28d0"
      },
      "source": [
        "# @title Visualise gridworlds { form-width: \"30%\" }\n",
        "\n",
        "# Instantiate two tabular environments, a simple task, and one that involves\n",
        "# the avoidance of an obstacle.\n",
        "simple_grid = build_gridworld_task(\n",
        "    task='simple', observation_type=ObservationType.GRID)\n",
        "obstacle_grid = build_gridworld_task(\n",
        "    task='obstacle', observation_type=ObservationType.GRID)\n",
        "\n",
        "# Plot them.\n",
        "simple_grid.plot_grid()\n",
        "plt.title('Simple')\n",
        "\n",
        "obstacle_grid.plot_grid()\n",
        "plt.title('Obstacle')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Obstacle')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPEAAADqCAYAAABz2qNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAJ4klEQVR4nO3dWYzddRXA8e9pO50udJtutKW0piAgpihVQohiIipgjCREJTGaiJrggxoSYww+4QsuIRrjg77ogxoEwTXRgGgiMSKiJaAESqSlUGhLl+lKp+2UOT7cy3QoU+jQuf3fM/P9JIS79fTHwHfuwnCIzERSXVOaPoCk02PEUnFGLBVnxFJxRiwVZ8RScUZcQETcGhE/b/gMf42Izzd5Bo3OiLtERHwmIv4bEYciYntE/DAi5p/mzMbjV+cZcReIiK8A3wa+CswDLgdWAfdHxPQmz6buZ8QNi4i5wDeAL2XmvZk5mJmbgU8Aq4FPtR86IyLuiogDEfFIRFwyYsbXIuKF9n1PRcRVEXEN8HXghog4GBGPtR97Y0Q82X7spoi46YTzXBcRj0bE/ojY2J4z2rk/256zJyLui4hV4/210akx4uZdAcwAfj3yxsw8CPwR+GD7puuAu4E+4A7gtxHRExEXAF8E3p2Zc4Crgc2ZeS9wG3BXZp6Vma9EvwP4CDAXuBH4XkRcChARlwE/pfWKYD5wJbD5xANHxHW0vkFcDywG/gb84rS/EnpTjLh5i4BdmXlslPu2te8HWJ+Z92TmIPBdWuFfDrwM9AJvi4iezNycmRtP9ptl5h8yc2O2PAD8CXhv++7PAT/JzPszcygzX8jMDaOM+QLwzcx8sn3u24B3+GzcDCNu3i5gUURMG+W+Ze37Aba8cmNmDgHPA8sz82ngZuBWYEdE3BkRy0/2m0XEtRHxUET0R8Re4MMc/0axEjjpN4ARVgHfj4i97Rn9QAArTuHXapwZcfP+ARyh9dJ0WEScBVwL/KV908oR900BzgG2AmTmHZn5HlpxJa0PyWhfHjmzF/gVcDuwNDPn03rJHu2HbAHWnMKZtwA3Zeb8EX/MzMwHT+0vWePJiBuWmftofbD1g4i4pv0+dzXwS1rPtj9rP3RdRFzffsa+mVb4D0XEBRHx/nagh4EBYKj9a14EVrejB5hO66X3TuBYRFwLfGjEcX4M3Nj+YGxKRKyIiAtHOfaPgFsi4mKAiJgXER8fhy+H3gQj7gKZ+R1aHxTdDuwH/knr2e6qzDzSftjvgBuAPcCngevb7497gW/Retm9HVgC3NL+NXe3/7w7Ih7JzAPAl2l9g9gDfBL4/YhzPEz7wy5gH/AArWf3E8/7G1rP9ndGxH7gcVqvGtSAcCmAVJvPxFJxRiwVZ8RScUYsFTfaDxic1KK+qbl6Zc+4HmD9f1ofvq5b21tibidnV5vbydnV5nZ69uYtg+zqfzlGu29Mn06/65IZ+fB9K9/4gWMwddnTALy87bwSczs5u9rcTs6uNrfTsy+7egv/fuzwqBH7cloqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4sa0KC8i/H++SA1Yt7bXRXnSRDWmvdPr1vbSqZW1H4iPjevcP+c9HZnbydnV5o6c3anVsn4tWi67estJ7/OZWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrObZdSAW67lCYwt1120exqmyOh3oZOt11K6jpGLBVnxFJxY3pPPNEcyL08wwYOsIfDDDCNafTQy1nM5WzOZUmsaPqIasChPMjzbKSfHQzwEskQPfQyk9n0sZRlnMvMmN30MYdN2oj35i7W8wDJ8X9rNshRBjnKIQ4wjR6WYMSTzdZ8lg2sZ4ihV91+hAGOMMBedjGFYDUXNnTC15q0EW9mw3DAa7mChSxhiCEOcZBdbGOK7zQmnf7cwRP8a/j6uZzPOaxhBrM4xiAH2MsOnmcKUxs85WtN2ogPcRCAafSwkKVMjalMBebRxzz6mj2cGvE0jw9fPoc1vDUuGb4+nV4WspSFLG3iaK9r0kY8g1kc4iDHGORB7mVRLmMefSxgcVe939GZcTQPs5/+4evd9HL5jUzaiFdyHv3sAFrvd15gEy+wCYB52ceFXMqcmN/kEXUGDXBo+PJUpjEjZg5ffyjv5yD7hq/3MJ33xUfP6Plez6R947c4lnMpV7KAxQSv/pHUffTzKH/nWB5r6HRq0on/PHS7SftMDNAXS+hjCYN5lH3sZifb2MozJMkRBtjH7q58D6TxN5NZw5ePMciRPExvzADg8vggcPzHKrvNpH0mPpaDw5d7YjqLYhkXxaUsZ/Xw7YMcbeBkasL0mMFcFgxff5anGjzN2EzaiB/jQR7Ph9mZWzmaRxjKIQ7mPvawa/gxs5nT4Al1pq3h7cOXn+N/bMonOJyHGMohXsoDDZ7s9U3al9NDDLGd59jOc6Pev5jlfrA1ySyMpVyU69jAIyTJJp5gE080faw3NGkjXsPF7GQr+9jNYQYY5AjBFGYxh6WcwyrOb/qIasCKeAvzcyFb2Eg/L3KYgfaPXU5nFnNYwGLOZnz/c9zTNWkjfuVDLelEs2MuF/LOpo9xyibte2JpojBiqTgjlopz26VUgNsupQnMbZddNLvT2y47sYWx2t8/t11K6jpGLBVnxFJxRiwVZ8RScUYsFWfEUnFGLBVnxFJxRiwVZ8RScUYsFWfEUnFGLBVnxFJxRiwVZ8RScUYsFWfEUnFGLBXnylqpAFfWShOYK2u7aLYra49zZe2rubJWmsCMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeLcdikV4LZLaQJz22UXza42t5Ozq80dOdttl5LGxIil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzm2XUgFuu5QmMLdddtHsanM7Obva3JGz3XYpaUyMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOlbVSAa6slSYwV9Z20exqczs5u9rckbNdWStpTIxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4oxYKs6IpeKMWCrOiKXijFgqzoil4tx2KRXgtktpAuuabZfjvSGwU3M7Obva3E7Orja307PddilNYEYsFWfEUnFGLBVnxFJxRiwVZ8RScUYsFWfEUnFGLBVnxFJxRiwVZ8RScUYsFWfEUnFGLBVnxFJxRiwVZ8RScUYsFTfWlbU7gWc7dxxJJ7EqMxePdseYIpbUfXw5LRVnxFJxRiwVZ8RScUYsFWfEUnFGLBVnxFJxRiwV93/IaFZBTZxz8wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJlbdgTypkYl"
      },
      "source": [
        "In this environment, the agent has four possible  <font color='blue'>**Actions**</font>: `up`, `right`, `down`, and `left`.  <font color='green'>**Reward**</font> is `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise. The episode ends when the agent reaches the goal, and otherwise continues. **Discount** on continuing steps, is $\\gamma = 0.9$. \n",
        "\n",
        "Before we start building an agent to interact with this environment, let's first look at the types of objects the environment either returns (e.g. observations) or consumes (e.g. actions). The `environment_spec` will show you the form of the *observations*, *rewards* and *discounts* that the environment exposes and the form of the *actions* that can be taken."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYa7sOBFpqwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d976864-8bfd-4c0a-b2b9-0b31a18077f4"
      },
      "source": [
        "environment, environment_spec = setup_environment(simple_grid)\n",
        "\n",
        "print('actions:\\n', environment_spec.actions, '\\n')\n",
        "print('observations:\\n', environment_spec.observations, '\\n')\n",
        "print('rewards:\\n', environment_spec.rewards, '\\n')\n",
        "print('discounts:\\n', environment_spec.discounts, '\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actions:\n",
            " DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=3, num_values=4) \n",
            "\n",
            "observations:\n",
            " Array(shape=(9, 10, 3), dtype=dtype('float32'), name='observation_grid') \n",
            "\n",
            "rewards:\n",
            " Array(shape=(), dtype=dtype('float32'), name='reward') \n",
            "\n",
            "discounts:\n",
            " BoundedArray(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuKc-xawpstY"
      },
      "source": [
        "We first set the environment to its initial location by calling the `reset` method which returns the first observation. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCw43t3hpujf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "77c32f4e-c63f-4823-97ed-ac637aafd47f"
      },
      "source": [
        "environment.reset()\n",
        "environment.plot_state()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['symbola'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si0_DxTqpxKX"
      },
      "source": [
        "Now we want to take an action using the `step` method to interact with the environment which returns a `TimeStep` \n",
        "namedtuple with fields:\n",
        "\n",
        "```none\n",
        "step_type, reward, discount, observation\n",
        "``` \n",
        "\n",
        "We can then visualise the updated state of the grid. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wv2MhUppzfE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2405f296-2fcf-4a5f-a0bb-fdb68bfd6f06"
      },
      "source": [
        "timestep = environment.step(1)\n",
        "environment.plot_state()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTvZorvVEldv",
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions for visualisation  { form-width: \"30%\" }\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_state_value(action_values, epsilon=0.1):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "def plot_action_values(action_values, epsilon=0.1):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  dif = vmax - vmin\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    \n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "      \n",
        "\n",
        "def smooth(x, window=10):\n",
        "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)\n",
        "  \n",
        "def plot_stats(stats, window=10):\n",
        "  plt.figure(figsize=(16,4))\n",
        "  plt.subplot(121)\n",
        "  xline = range(0, len(stats.episode_lengths), window)\n",
        "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
        "  plt.ylabel('Episode Length')\n",
        "  plt.xlabel('Episode Count')\n",
        "  plt.subplot(122)\n",
        "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
        "  plt.ylabel('Episode Return')\n",
        "  plt.xlabel('Episode Count')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtlEEWqSlq5"
      },
      "source": [
        "# 2. Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh-qQtBcS28_"
      },
      "source": [
        "Now that we've defined an environment, we would like to define an agent to interact with it. An agent should be able to do three things: act, according to its previous observations, observe environment timesteps (which contain observations, but also other information useful for learning, such as instantaneous rewards or discount factors). ACME direcly provide an <a href=\"https://github.com/deepmind/acme/blob/dbdbe5f12c740242b8504b9426e52a239dace071/acme/core.py#L34\">API for agents</a>, which encapsulate this logic.\n",
        "\n",
        "The `Actor` class has three main methods:\n",
        "- `select_action` which defines the action selection process of the agent, based on the current observation.\n",
        "- `observe` which takes in the current timestep, and extract the useful information (potentially placing it inside of a replay buffer).\n",
        "- `update` which updates the behavior of the agent (typically by performing gradient steps on the agent parameters).\n",
        "\n",
        "`Actor` also exposes an `observe_first` method, which specializes the `observe` method for the first observation in an episode.\n",
        "\n",
        "Take a detailled look at the API before proceeding, we will use it to implement our DQN agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3kKMKZXRn84"
      },
      "source": [
        "To facilitate training and evaluation, we directly provide a run loop, which is going to take as input an ACME `agent` and an `environment`, and is going to run the training procedure of the agent on the environment for a certain number of episodes.\n",
        "\n",
        "We also define an evaluation loop, which also takes as input an `agent` and an `environment`, runs the agent on the environment in evaluation mode, for a certain number of episodes, and outputs corresponding frames, that can then be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiUu93VMuj38",
        "cellView": "form"
      },
      "source": [
        "#@title Run loop  { form-width: \"30%\" }\n",
        "\n",
        "\n",
        "def run_loop(environment,\n",
        "             agent,\n",
        "             num_episodes=None,\n",
        "             num_steps=None,\n",
        "             logger_time_delta=1.,\n",
        "             label='training_loop',\n",
        "             log_loss=False,\n",
        "             ):\n",
        "  \"\"\"Perform the run loop.\n",
        "\n",
        "  We are following the Acme run loop.\n",
        "\n",
        "  Run the environment loop for `num_episodes` episodes. Each episode is itself\n",
        "  a loop which interacts first with the environment to get an observation and\n",
        "  then give that observation to the agent in order to retrieve an action. Upon\n",
        "  termination of an episode a new episode will be started. If the number of\n",
        "  episodes is not given then this will interact with the environment\n",
        "  infinitely.\n",
        "\n",
        "  Args:\n",
        "    environment: dm_env used to generate trajectories.\n",
        "    agent: acme.Actor for selecting actions in the run loop.\n",
        "    num_steps: number of episodes to run the loop for. If `None` (default), runs\n",
        "      without limit.\n",
        "    num_episodes: number of episodes to run the loop for. If `None` (default),\n",
        "      runs without limit.\n",
        "    logger_time_delta: time interval (in seconds) between consecutive logging\n",
        "      steps.\n",
        "    label: optional label used at logging steps.\n",
        "  \"\"\"\n",
        "  logger = loggers.TerminalLogger(label=label, time_delta=logger_time_delta)\n",
        "  iterator = range(num_episodes) if num_episodes else itertools.count()\n",
        "  all_returns = []\n",
        "  \n",
        "  num_total_steps = 0\n",
        "  for episode in iterator:\n",
        "    # Reset any counts and start the environment.\n",
        "    start_time = time.time()\n",
        "    episode_steps = 0\n",
        "    episode_return = 0\n",
        "    episode_loss = 0\n",
        "\n",
        "    timestep = environment.reset()\n",
        "    \n",
        "    # Make the first observation.\n",
        "    agent.observe_first(timestep)\n",
        "\n",
        "    # Run an episode.\n",
        "    while not timestep.last():\n",
        "      # Generate an action from the agent's policy and step the environment.\n",
        "      action = agent.select_action(timestep.observation)\n",
        "      timestep = environment.step(action)\n",
        "\n",
        "      # Have the agent observe the timestep and let the agent update itself.\n",
        "      agent.observe(action, next_timestep=timestep)\n",
        "      agent.update()\n",
        "\n",
        "      # Book-keeping.\n",
        "      episode_steps += 1\n",
        "      num_total_steps += 1\n",
        "      episode_return += timestep.reward\n",
        "\n",
        "      if log_loss:\n",
        "        episode_loss += agent.last_loss\n",
        "\n",
        "      if num_steps is not None and num_total_steps >= num_steps:\n",
        "        break\n",
        "\n",
        "    # Collect the results and combine with counts.\n",
        "    steps_per_second = episode_steps / (time.time() - start_time)\n",
        "    result = {\n",
        "        'episode': episode,\n",
        "        'episode_length': episode_steps,\n",
        "        'episode_return': episode_return,\n",
        "    }\n",
        "    if log_loss:\n",
        "      result['loss_avg'] = episode_loss/episode_steps\n",
        "\n",
        "    all_returns.append(episode_return)\n",
        "\n",
        "    # Log the given results.\n",
        "    logger.write(result)\n",
        "    \n",
        "    if num_steps is not None and num_total_steps >= num_steps:\n",
        "      break\n",
        "  return all_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A29muHc5Mybf",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluation loop { form-width: \"30%\" }\n",
        "\n",
        "def evaluate(environment, agent, evaluation_episodes):\n",
        "  frames = []\n",
        "\n",
        "  for episode in range(evaluation_episodes):\n",
        "    timestep = environment.reset()\n",
        "    episode_return = 0\n",
        "    steps = 0\n",
        "    while not timestep.last():\n",
        "      frames.append(environment.plot_state(return_rgb=True))\n",
        "\n",
        "      action = agent.select_action(timestep.observation)\n",
        "      timestep = environment.step(action)\n",
        "      steps += 1\n",
        "      episode_return += timestep.reward\n",
        "    print(\n",
        "        f'Episode {episode} ended with reward {episode_return} in {steps} steps'\n",
        "    )\n",
        "  return frames\n",
        "\n",
        "def display_video(frames, filename='temp.mp4', frame_repeat=1):\n",
        "  \"\"\"Save and display video.\"\"\"\n",
        "  # Write video\n",
        "  with imageio.get_writer(filename, fps=60) as video:\n",
        "    for frame in frames:\n",
        "      for _ in range(frame_repeat):\n",
        "        video.append_data(frame)\n",
        "  # Read video and display the video\n",
        "  video = open(filename, 'rb').read()\n",
        "  b64_video = base64.b64encode(video)\n",
        "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
        "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
        "  return IPython.display.HTML(video_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj6aTY1AIKVg"
      },
      "source": [
        "### ***Exercise***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn2m-pk3oTHN"
      },
      "source": [
        "As a first exercise, define a random agent (i.e. an agent that outputs uniformly random actions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCTcUvz9ofJN",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** Random agent { form-width: \"30%\" }\n",
        "\n",
        "class RandomAgent(acme.Actor):\n",
        "  def __init__(self, environment_spec: specs.EnvironmentSpec) -> None:\n",
        "    action_spec = environment_spec.actions\n",
        "    assert isinstance(action_spec, dm_env.specs.DiscreteArray)\n",
        "    self._num_values = self.environment_spec.action.num_values\n",
        "\n",
        "  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n",
        "    \"\"\"Samples from the policy and returns an action.\"\"\"\n",
        "    ##### IMPLEMENT #####\n",
        "\n",
        "  def observe_first(self, timestep: dm_env.TimeStep):\n",
        "    \"\"\"Make a first observation from the environment.\n",
        "    Note that this need not be an initial state, it is merely beginning the\n",
        "    recording of a trajectory.\n",
        "    Args:\n",
        "      timestep: first timestep.\n",
        "    \"\"\"\n",
        "    ##### IMPLEMENT #####\n",
        "\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      action: types.NestedArray,\n",
        "      next_timestep: dm_env.TimeStep,\n",
        "  ):\n",
        "    \"\"\"Make an observation of timestep data from the environment.\n",
        "    Args:\n",
        "      action: action taken in the environment.\n",
        "      next_timestep: timestep produced by the environment given the action.\n",
        "    \"\"\"\n",
        "    ##### IMPLEMENT #####\n",
        "\n",
        "\n",
        "  def update(self, wait: bool = False):\n",
        "    \"\"\"Perform an update of the actor parameters from past observations.\n",
        "    Args:\n",
        "      wait: if True, the update will be blocking.\n",
        "    \"\"\"\n",
        "    ##### IMPLEMENT #####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlhfTQlNrPP-",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** Random agent { form-width: \"30%\" }\n",
        "\n",
        "class RandomAgentSolution(acme.Actor):\n",
        "  def __init__(self, environment_spec: specs.EnvironmentSpec) -> None:\n",
        "    action_spec = environment_spec.actions\n",
        "    assert isinstance(action_spec, dm_env.specs.DiscreteArray)\n",
        "    self._num_values = action_spec.num_values\n",
        "\n",
        "  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n",
        "    \"\"\"Samples from the policy and returns an action.\"\"\"\n",
        "    return np.random.randint(low=0, high=self._num_values)\n",
        "\n",
        "  def observe_first(self, timestep: dm_env.TimeStep):\n",
        "    \"\"\"Make a first observation from the environment.\n",
        "    Note that this need not be an initial state, it is merely beginning the\n",
        "    recording of a trajectory.\n",
        "    Args:\n",
        "      timestep: first timestep.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      action: types.NestedArray,\n",
        "      next_timestep: dm_env.TimeStep,\n",
        "  ):\n",
        "    \"\"\"Make an observation of timestep data from the environment.\n",
        "    Args:\n",
        "      action: action taken in the environment.\n",
        "      next_timestep: timestep produced by the environment given the action.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def update(self, wait: bool = False):\n",
        "    \"\"\"Perform an update of the actor parameters from past observations.\n",
        "    Args:\n",
        "      wait: if True, the update will be blocking.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITkuANeIsEIq"
      },
      "source": [
        "Replace the `RandomAgentSolution` by your `RandomAgent` to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ESGYsnYrH5n",
        "outputId": "a621f8ce-6bcf-4cb9-ea7d-96978c22f27b"
      },
      "source": [
        "run_loop(agent=RandomAgentSolution(environment_spec), environment=environment, num_episodes=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-235.0, -10.0, -95.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOeElBpmsYFo"
      },
      "source": [
        "# 3. Introduction to JAX and Haiku"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03tnkC41spVL"
      },
      "source": [
        "To implement our simple DQN, we will resort to neural networks as our function approximators, and we will train them, using gradient based optimizers. To make that easy, we will use <a href=\"https://github.com/google/jax\">JAX</a>, to get access to easy gradient computations with a numpy-like flavor, and <a href=\"https://github.com/deepmind/dm-haiku\">Haiku</a>, to easily define our neural network architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB2JZswysbdh"
      },
      "source": [
        "## 3.1. JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXqsxL35udKp"
      },
      "source": [
        "JAX is a numerical computation library, very close to numpy for its basic use, allows one to easily execute operations on GPU, and gives access to <a href=\"https://github.com/google/jax#transformations\">numerical function transformations</a>, that allows for gradient computations, automatic vectorization, or jitting.\n",
        "\n",
        "JAX has a _functional_ flavor; to be able to use numerical function transformations, you will have to define _pure_ functions, i.e. mathematical functions, whose result do not depend on the context in which they are used.\n",
        "\n",
        "For instance the following function is pure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7avvHR67yA11"
      },
      "source": [
        "def pure_function(x: chex.Array) -> chex.Array:\n",
        "  return 3 * x + jnp.tanh(2 * x) / (x ** 2 + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-lybNOy481"
      },
      "source": [
        "The following method is not pure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D3KcpERzJqJ"
      },
      "source": [
        "class Counter:\n",
        "  def __init__(self) -> None:\n",
        "    self._i = 0.\n",
        "\n",
        "  def unpure_function(self, x: chex.Array) -> chex.Array:\n",
        "    self._i = self._i + 1.\n",
        "    return self._i * x + jnp.tanh(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrLn-PEAz57d"
      },
      "source": [
        "Given a pure function, you can easily obtain the associated gradient function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU0Jh8Za0DL_",
        "outputId": "f323f5d7-3a34-4e18-f8eb-c44e495dff32"
      },
      "source": [
        "grad_pure = jax.grad(pure_function)\n",
        "x = 3.\n",
        "print(f'Value at point x={x}, f(x)={pure_function(x)}, grad_f(x)={grad_pure(x)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Value at point x=3.0, f(x)=9.099998474121094, grad_f(x)=2.9400057792663574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeUZPP0nF53M"
      },
      "source": [
        "In addition to `jax.grad`, JAX provides `jax.vmap` for automatic vectorization, `jax.jit` for jitting (to fully make use of specialized hardware) and `jax.pmap`, to automatically distribute functions accross devices.\n",
        "\n",
        "For instance, if you want to have a batched version of matrix multiplication, you can use the usual matrix multiplication, and directly vmap it in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3KFxDh3Tt2m",
        "outputId": "29d6884f-c541-43b2-f173-b1106cd17c70"
      },
      "source": [
        "batch_matrix_multiply = jax.vmap(lambda a, b: a @ b)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng_a, rng_b = jax.random.split(rng)\n",
        "a = jax.random.normal(key=rng_a, shape=(12, 5, 7))\n",
        "b = jax.random.normal(key=rng_b, shape=(12, 7, 9))\n",
        "print(batch_matrix_multiply(a, b).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12, 5, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcpax51lUnhP"
      },
      "source": [
        "In this example, we have been hitting one of the differences between JAX numpy and numpy. Numpy handles random seeds _implicitly_, when you want a random\n",
        "number, you get one by simply calling one of numpy's functions, and the number\n",
        "will depend on numpy global seed. With JAX, random seeds are handled _explicitly_, and each function that needs to generate random numbers takes a random key as additional input. This has to do with the functional paradigm of JAX: if we were not handling the random key explicitly, each call to a random function would lead to a different result, breaking the pure function hypothesis. By passing the random key explicitly, we make sure that the same random function, called with the same random key, will produce the same result. As a side effect, this also make results produced using JAX easily reproducible, as it is easy to trace which random seeds have been used where. To\n",
        "know more about how JAX handles randomness, you can read <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#rngs-and-state\">this page<a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX0_U5lHZuCV"
      },
      "source": [
        "### ***Exercises***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnMpTZq_Zwy5"
      },
      "source": [
        "As a first exercise, we are giving you the following function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9903KIgZs3J"
      },
      "source": [
        "def func(x: chex.Array) -> chex.Array:\n",
        "  return x ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugrEP9JIZ630"
      },
      "source": [
        "which simply computes the square of an array. By using simple jax transformations can you get a function that takes a batch of scalars, and outputs the value of the gradient of the squared function for each element of the batch?\n",
        "As a hint: jax.grad can only take as input a function that outputs a single scalar, so calling jax.grad directly on func and applying it to a vector won't work.\n",
        "Can you make it run faster?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nj1m2vxahuG",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** Batched gradients { form-width: \"30%\" }\n",
        "\n",
        "batched_grad = None\n",
        "fast_batched_grad = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2-_WBtvajb1",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** Batched gradients { form-width: \"30%\" }\n",
        "solution_batched_grad = jax.vmap(jax.grad(func))\n",
        "jitted_solution = jax.jit(solution_batched_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkp0HdK3b0sT"
      },
      "source": [
        "You can test your solution by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV8aarewbuNp",
        "cellView": "form",
        "outputId": "bd6710ce-9026-4cb4-cc71-d9db6c3c68ff"
      },
      "source": [
        "#@title **[Test]** Batched gradients { form-width: \"30%\" }\n",
        "key = jax.random.PRNGKey(0)\n",
        "normal = jax.random.normal(key=key, shape=(3,))\n",
        "if (jitted_solution(normal) == 2 * normal).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Provably incorrect.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probably correct.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kiq645RbePv"
      },
      "source": [
        "Can you do the same for a batch of batches, without flattening your input? (i.e. you have a matrix of numbers, and you want a matrix containing the gradient for each of the numbers in the matrix.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcMNaIlBc1RD",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** Matrix gradients { form-width: \"30%\" }\n",
        "fast_matrix_grad = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxYugrUvdSMn",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** Matrix gradients { form-width: \"30%\" }\n",
        "jitted_solution_matrix = jax.jit(jax.vmap(jitted_solution))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq5_gmUkdZ1N"
      },
      "source": [
        "#@title **[Test]** Matrix gradients { form-width: \"30%\" }\n",
        "key = jax.random.PRNGKey(0)\n",
        "normal = jax.random.normal(key=key, shape=(3, 3,))\n",
        "if (fast_matrix_grad(normal) == 2 * normal).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Provably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0v4pUxXduJV"
      },
      "source": [
        "Another very useful application of `vmap` is batched indexing. Assume you have a `[B1, B2, ..., BN]` tensor of indices `idx`, and a `[B1, B2, ..., BN, F]` tensor of features `features`, and for each element `i1, ..., iN`, you\n",
        "would like to  retrieve element `features[i1, ..., iN, idx[i1, ..., iN]]` from\n",
        "the feature tensor, can you do this easily using vmap? (maybe start with a fixed `N`, then generalize to all `N`'s.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBlJnA9hAGQ",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** Batched indexing { form-width: \"30%\" }\n",
        "def batched_indexing(idxs: chex.Array, features: chex.Array) -> chex.Array:\n",
        "  ##### IMPLEMENT #####\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBjZTSw7e86s",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** Batched indexing { form-width: \"30%\" }\n",
        "@jax.jit\n",
        "def solution_batched_indexing(idxs: chex.Array, features: chex.Array) -> chex.Array:\n",
        "  def simple_index(idx, feature):\n",
        "    return feature[idx]\n",
        "  batched_index = simple_index\n",
        "  for _ in range(idxs.ndim):\n",
        "    batched_index = jax.vmap(batched_index)\n",
        "  return batched_index(idxs, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdb3DRDUgXKa",
        "cellView": "form"
      },
      "source": [
        "#@title **[Test]** Batched indexing { form-width: \"30%\" }\n",
        "inputs = jnp.array([[-0.196,  0.255,  0.573,  0.441, -0.847,  0.318,  0.646],\n",
        " [ 0.034, -0.889, -0.266, -1.561, -0.638, -0.442,  0.91 ],\n",
        " [-0.017,  0.758,  1.089,  0.299,  1.491,  0.079, -1.222],\n",
        " [ 0.952,  0.21,   1.386, -0.338,  2.952, -0.995, -0.516],\n",
        " [ 0.292, -0.143,  1.614,  1.643,  0.114,  0.254, -1.306],])\n",
        "outputs = jnp.array([ 0.255, -1.561,  0.079,  1.386, -1.306])\n",
        "idxs = jnp.array([1, 3, 5, 2, 6], dtype=jnp.int32)\n",
        "if (batched_indexing(idxs, inputs) == outputs).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Provably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIb4SMcWskTb"
      },
      "source": [
        "## 3.2. Haiku"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKKAY-RkiJ6w"
      },
      "source": [
        "Haiku is to JAX what Sonnet was to TensorFlow. It allows you to simply and cleanly define (deep) neural network architectures of all kind. If you know Sonnet, or Keras, or the standard PyTorch network building facilities, you won't be too surprised by how Haiku is defining networks. \n",
        "\n",
        "The main difference between Haiku and those other NN libraries is the transform mechanism. As mentionned earlier, JAX works with pure function. However, the way most NN libraries build NN architectures is impure (in a functional sense): for instance, for each `torch.Module`, PyTorch defines a `forward` function, that takes the inputs of the networks, and outputs its final activations. This `forward` function is impure: it relies on the parameters of the networks, which are attributes of the encompassing object, but not given as parameters to the function each time you call it. You need to know the state of the encompassing object to get the output of the function.\n",
        "\n",
        "On the other hand, to compute the gradient of a function, JAX requires a pure function, i.e. a function that takes both the _inputs_ of the network, as well as its _parameters_, and outputs the resulting activations. If you have such a function `f(params, inputs)`, it is straightforward to compute its gradient w.r.t. `params`, using `grad(f)(params, inputs)`. However, passing parameters around when defining network architectures would require a lot of boilerplate. To relieve the user from those considerations, Haiku allows user to define impure functions to define the architecture of the network, by only passing inputs around, then transform those impure functions into pure ones, by using `hk.transform`. Let's look at an example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzulGj1rmFr4"
      },
      "source": [
        "def easy_linear_net(x: chex.Array) -> chex.Array:\n",
        "  return hk.Linear(12)(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQTa5YWNmPRl"
      },
      "source": [
        "This defines a simple linear layer with 12 outputs. Similar to Keras and Sonnet, Haiku does not require you to specify the size of your input when defining linear or convolutional layers. This function is impure, as `hk.Linear` implicitly defines some parameters. Directly calling it won't work and haiku will raise an exception saying you did not transform the function first. To be able to apply the module you have created, you need first to wrap it inside a `hk.transform` in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPh6ynXm6aAL"
      },
      "source": [
        "ez_linear = hk.transform(easy_linear_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLpgeDXu6m72"
      },
      "source": [
        "Once transformed, `ez_linear` contains two functions, `ez_linear.init`, which takes in a random seed and an input for the network and outputs a set of parameters for your module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vzvZavQ6l2K"
      },
      "source": [
        "ez_linear_params = ez_linear.init(rng=jax.random.PRNGKey(0), x=jnp.zeros((1, 6)))\n",
        "print(ez_linear_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMbdnf1S7sbH"
      },
      "source": [
        "Once you have your parameters, you can use your transformed function apply method `ez_linear.apply`, which takes in the parameters of the networks, a random seed (to cover the case of the haiku module being stochastic), and an input for the network, to get your outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvFiBHsa8CWf"
      },
      "source": [
        "outputs = ez_linear.apply(params=ez_linear_params, rng=jax.random.PRNGKey(0), x=jnp.ones((1, 6)))\n",
        "print(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tehhJADi8l7X"
      },
      "source": [
        "You can then use your apply function as you would any other pure function and compute gradients through it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHJJpTq88kzp"
      },
      "source": [
        "def loss_fn(params: hk.Params, rng: chex.PRNGKey, inputs: chex.Array) -> chex.Array:\n",
        "  return ez_linear.apply(params, rng, inputs).sum()\n",
        "\n",
        "grad_fn = jax.grad(loss_fn)\n",
        "print(grad_fn(ez_linear_params, rng, jnp.ones((1, 6))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVIEfsDg98F4"
      },
      "source": [
        "### ***Exercises***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdsmU3m1-Nxb"
      },
      "source": [
        "Using the linear module and the parameters defined above, fill in the following code to perform linear regression on the given dataset. You should get to a final training loss of about .94:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnhwkzJS-s76",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** Linear regression { form-width: \"30%\" }\n",
        "\n",
        "inputs = jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 6))\n",
        "outputs = 12 * jnp.concatenate([inputs, inputs], axis=-1) + 6 + jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 12))\n",
        "\n",
        "rng, cur_rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "ez_linear_params = ez_linear.init(rng=cur_rng, x=jnp.zeros((1, 6)))\n",
        "\n",
        "learning_rate = 1e-1\n",
        "num_iterations = 500\n",
        "\n",
        "def loss_fn(params: hk.Params, rng: chex.PRNGKey, inputs: chex.Array, targets: chex.Array) -> chex.Array:\n",
        "  # Implement: Fill in the loss function\n",
        "  pass\n",
        "\n",
        "# Implement: Define your grad function\n",
        "value_and_grad_fn = ...\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  if i % 100 == 99:\n",
        "    print(f'Loss at iteration {i}: {loss}')\n",
        "  cur_rng, rng = jax.random.split(rng)\n",
        "  # Implement: Call the grad function, and compute the loss function\n",
        "  loss, grad = ...\n",
        "\n",
        "  # This performs a gradient step. As the gradient and parameters are nested dictionaries, \n",
        "  # we have to perform the step at each of the leaf node of the nested structure.\n",
        "  ez_linear_params = jax.tree_multimap(lambda p, g: p - learning_rate * g, ez_linear_params, grad)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyok2A-NCEmz",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** Linear regression { form-width: \"30%\" }\n",
        "inputs = jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 6))\n",
        "outputs = 12 * jnp.concatenate([inputs, inputs], axis=-1) + 6 + jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 12))\n",
        "\n",
        "rng, cur_rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "ez_linear_params = ez_linear.init(rng=cur_rng, x=jnp.zeros((1, 6)))\n",
        "\n",
        "learning_rate = 1e-1\n",
        "num_iterations = 500\n",
        "\n",
        "def loss_fn(params: hk.Params, rng: chex.PRNGKey, inputs: chex.Array, targets: chex.Array) -> chex.Array:\n",
        "  preds = ez_linear.apply(params, rng, inputs)\n",
        "  return jnp.mean((preds - targets) ** 2)\n",
        "\n",
        "value_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\n",
        "for i in range(num_iterations):\n",
        "  if i % 100 == 99:\n",
        "    print(f'Loss at iteration {i}: {loss}')\n",
        "  cur_rng, rng = jax.random.split(rng)\n",
        "  loss, grad = value_and_grad_fn(ez_linear_params, cur_rng, inputs, outputs)\n",
        "  ez_linear_params = jax.tree_multimap(lambda p, g: p - learning_rate * g, ez_linear_params, grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtkGek9pNOnR"
      },
      "source": [
        "To define our Deep-Q learning, we will put a convnet on top of our observations to approximate the Q-function. This convnet takes as input a `(9, 10, 3)` observation tensor, and should output a `(num_actions,)` tensor. Implement such a convnet. To keep things simple, we will use 2 layers of convolutions, with kernel shapes respectively 4 and 3, number of channels 32 and 64, strides 2 and 1, and VALID padding, followed by a flattening layer and 2 layers of MLP, both of size 50. We will put a relu activation between each hidden layer.\n",
        "\n",
        "Verify that your convnet is outputting tensors of the correct shape on the dummy observations provided by the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_tml1A1P5Hp",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** ConvNet{ form-width: \"30%\" }\n",
        "def network(x):\n",
        "  ##### IMPLEMENT ####\n",
        "  pass\n",
        "\n",
        "net_init, net_apply = ... #### IMPLEMENT\n",
        "net_output = .... ##### IMPLEMENT\n",
        "\n",
        "if net_output.shape == (1, 4):\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Provably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z6x7lbEP3B5",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** ConvNet{ form-width: \"30%\" }\n",
        "def network_solution(x):\n",
        "  model = hk.Sequential([\n",
        "      hk.Conv2D(32, kernel_shape=[4,4], stride=[2,2], padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(64, kernel_shape=[3,3], stride=[1,1], padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Flatten(),\n",
        "      hk.nets.MLP([50, 50, environment_spec.actions.num_values])\n",
        "  ])\n",
        "  return model(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fHHwGDZYI1CJ"
      },
      "source": [
        "#@title **[Test]** ConvNet{ form-width: \"30%\" }\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "net_init, net_apply = hk.without_apply_rng(hk.transform(network))\n",
        "net_params = net_init(rng, timestep.observation)\n",
        "net_output = net_apply(netsol_params, timestep.observation)\n",
        "\n",
        "if net_output.shape == (1, 4):\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Provably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHPOBsmvBTKz"
      },
      "source": [
        "# 4. Implementing DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO5NSfxvBW2N"
      },
      "source": [
        "We are now ready to get to the DQN implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47TpzYYNBlTy",
        "cellView": "form"
      },
      "source": [
        "#@title Create the environment { form-width: \"30%\" }\n",
        "grid = build_gridworld_task(\n",
        "    task='simple', \n",
        "    observation_type=ObservationType.GRID,\n",
        "    max_episode_length=200)\n",
        "environment, environment_spec = setup_environment(grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n98Ll4VZKcPB"
      },
      "source": [
        "## 4.1. ACME DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON3SPOc6B6F_"
      },
      "source": [
        "Before we get to a reimplementation, we are going to have a look at how the `acme.dqn` implementation does on our environment. Use the acme implementation of DQN that can be found <a href=\"https://github.com/deepmind/acme/blob/master/acme/agents/jax/dqn/agent.py\">here</a>, and plug it inside the `run_loop` function defined earlier to train a DQN agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Tdxl3LpiKDL0"
      },
      "source": [
        "#@title **[Implement]** ACME DQN { form-width: \"30%\" }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG4ejfr7Ct44",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** ACME DQN { form-width: \"30%\" }\n",
        "solution_agent = dqn.DQN(\n",
        "    environment_spec=environment_spec,\n",
        "      network=hk.without_apply_rng(hk.transform(network)),\n",
        "      batch_size=10,\n",
        "      samples_per_insert=2,\n",
        "      min_replay_size=10,\n",
        "      epsilon=0.05,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAq9s3lHENtt",
        "cellView": "form"
      },
      "source": [
        "#@title **[Running]** ACME DQN { form-width: \"30%\" }\n",
        "returns = run_loop(environment=environment, agent=agent, num_episodes=100, num_steps=100000, logger_time_delta=0.2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzDaGBJEEWzX",
        "cellView": "form"
      },
      "source": [
        "# @title **[Visualising]** Learned Q values { form-width: \"30%\" }\n",
        "\n",
        "# get agent parameters\n",
        "params = agent._learner.get_variables([])[0]\n",
        "\n",
        "# Evaluate the policy for every state, similar to tabular agents above.\n",
        "pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
        "q = np.zeros(grid._layout_dims + (4,))\n",
        "for y in range(grid._layout_dims[0]):\n",
        "  for x in range(grid._layout_dims[1]):\n",
        "    # Hack observation to see what the Q-network would output at that point.\n",
        "    environment.set_state(x, y)\n",
        "    obs = environment.get_obs()\n",
        "    q[y, x] = np.asarray(agent._learner._forward(params, np.expand_dims(obs, axis=0)))\n",
        "    pi[y, x] = np.asarray(agent.select_action(obs))\n",
        "    \n",
        "plot_action_values(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ftjos6Kqod"
      },
      "source": [
        "## 4.2. Your own DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM-fil5s1AYq"
      },
      "source": [
        "We will now try to implement our own version of Deep Q-learning. To do this we will first implement a `DQNLearner` class, that takes care of all the learning/updating part of DQN, then plug it inside a `DQNAgent`, that implements all the facilities to make the `DQNLearner` work properly (notably handling the replay buffer).\n",
        "\n",
        "The DQN learner implements a loss function which is used by the learner class to compute gradients for the parameters $\\theta_i$ of the Q-network $Q( \\cdot; \\theta_i)$:\n",
        "\n",
        "```none\n",
        "loss(params: hk.Params, target_params: hk.Params, sample: reverb.ReplaySample)\n",
        "```\n",
        "which, at iteration `i` computes the DQN loss $L_i$ on the parameters $\\theta_i$, based on a the set of target parameters $\\theta_{i-1}$ and a given batch of sampled trajectories `sample`. As described in the manuscript, the loss function is defined as:\n",
        "\n",
        "$$L_i (\\theta_i) = \\mathbb{E}_{\\color{red}{s},\\color{blue}{a} \\sim \\rho(\\cdot)} \\left[ \\left( y_i - Q(\\color{red}{s},\\color{blue}{a} ;\\theta_i) \\right)^2\\right]$$\n",
        "\n",
        "where the target $y_i$ is computed using a bootstrap value computed from Q-value network with target parameters:\n",
        "\n",
        "$$ y_i = \\mathbb{E}_{\\color{red}{s'} \\sim \\mathcal{E}} \\left[ \\color{green}{r} + \\gamma \\max_{\\color{blue}{a'} \\in \\color{blue}{\\mathcal{A}}} Q(\\color{red}{s'}, \\color{blue}{a'} ; \\theta^{\\text{target}}_i) \\; | \\; \\color{red}{s}, \\color{blue}{a} \\right] $$\n",
        "\n",
        "The batch of data `sample` is prepackaged by the agent to match the sampling distributions $\\rho$ and $\\mathcal{E}$. To get the explicit data items, use the following:\n",
        "\n",
        "```none\n",
        "o_tm1, a_tm1, r_t, d_t, o_t = sample.data\n",
        "```\n",
        "\n",
        "The function is expected to return  \n",
        "* `mean_loss` is the mean of the above loss over the batched data,\n",
        "* (`keys`, `priorities`) will pair the `keys` corresponding to each batch item to the absolute TD-error used to compute the `mean_loss` above. The agent uses these to update priorities for samples in the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DCGq1HFRiq",
        "cellView": "form"
      },
      "source": [
        "# @title **[Implement]** DQN Learner  { form-width: \"30%\" }\n",
        "TrainingState =  namedtuple('TrainingState', 'params, target_params, opt_state, step')\n",
        "LearnerOutputs =  namedtuple('LearnerOutputs', 'keys, priorities')\n",
        "\n",
        "class DQNLearner(acme.Learner):\n",
        "  \"\"\"DQN learner.\"\"\"\n",
        "\n",
        "  _state: TrainingState\n",
        "\n",
        "  def __init__(self,\n",
        "               network,\n",
        "               obs_spec,\n",
        "               discount,\n",
        "               importance_sampling_exponent,\n",
        "               target_update_period,\n",
        "               data_iterator,\n",
        "               optimizer,\n",
        "               rng,\n",
        "               replay_client,\n",
        "               max_abs_reward=1.,\n",
        "               huber_loss_parameter=1.,\n",
        "               ):\n",
        "    \"\"\"Initializes the learner.\"\"\"\n",
        "\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Use provided params to initialize any jax functions used in the `step`\n",
        "    # function.\n",
        "    \n",
        "    # Internalise agent components (replay buffer, networks, optimizer).\n",
        "    self._replay_client = replay_client\n",
        "    self._iterator = data_iterator\n",
        "\n",
        "    # Since sampling is base on a priority experience replay, we need to pass\n",
        "    # the absolute td-loss values to the replay client to update priorities\n",
        "    # accordingly.\n",
        "    def update_priorities(outputs: LearnerOutputs):\n",
        "      for key, priority in zip(outputs.keys, outputs.priorities):\n",
        "        self._replay_client.mutate_priorities(\n",
        "            table='priority_table', \n",
        "            updates={key: priority})\n",
        "        \n",
        "    self._update_priorities = update_priorities\n",
        "\n",
        "    # Internalise the hyperparameters.\n",
        "    self._target_update_period = target_update_period\n",
        "\n",
        "    # Internalise logging/counting objects.\n",
        "    self._counter = counting.Counter()\n",
        "    self._logger = loggers.TerminalLogger('learner', time_delta=1.)\n",
        "\n",
        "    # Initialise parameters and optimiser state.\n",
        "\n",
        "    # Transform network into a pure function.\n",
        "    network = hk.transform(network)\n",
        "\n",
        "    def initialization_fn(values):\n",
        "      values = tree_util.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), values)\n",
        "      # Add batch dim.\n",
        "      return tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), values)\n",
        "\n",
        "    initial_params = network.init(next(rng), initialization_fn(obs_spec))\n",
        "    initial_target_params = network.init(next(rng), initialization_fn(obs_spec))\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "\n",
        "    self._state = TrainingState(\n",
        "        params=initial_params,\n",
        "        target_params=initial_target_params,\n",
        "        opt_state=initial_opt_state,\n",
        "        step=0)\n",
        "    \n",
        "  def loss(params: hk.Params, target_params: hk.Params,\n",
        "             sample: reverb.ReplaySample):\n",
        "      o_tm1, a_tm1, r_t, d_t, o_t = sample.data\n",
        "      keys, probs = sample.info[:2]\n",
        "      \n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # return mean_loss, (keys, priorities)\n",
        "\n",
        "      pass\n",
        "      \n",
        "\n",
        "  def sgd_step(state, samples):\n",
        "    # Compute gradients on the given loss function and update the network\n",
        "    # using the optimizer provided at init time.\n",
        "    grad_fn = jax.grad(loss, has_aux=True)\n",
        "    gradients, (keys, priorities) = grad_fn(state.params, state.target_params,\n",
        "                                            samples)\n",
        "    updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
        "    new_params = optix.apply_updates(state.params, updates)\n",
        "\n",
        "    # Update the internal state for the learner with (1) network parameters,\n",
        "    # (2) parameters of the target network, (3) the state of the optimizer,\n",
        "    # (4) Numbers of SGD steps performed by the agent.  \n",
        "    new_state = TrainingState(\n",
        "        params=new_params,\n",
        "        target_params=state.target_params,\n",
        "        opt_state=new_opt_state,\n",
        "        step=state.step + 1)\n",
        "\n",
        "    outputs = LearnerOutputs(keys=keys, priorities=priorities)\n",
        "    return new_state, outputs\n",
        "\n",
        "  def step(self):\n",
        "    samples = next(self._iterator)\n",
        "    \n",
        "    # Do a batch of SGD and update self._state accordingly.\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    \n",
        "    # Update our counts and record it.\n",
        "    result = self._counter.increment(steps=1)\n",
        "\n",
        "    # Periodically update target network parameters.\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    \n",
        "    # Update priorities in replay.\n",
        "    self._update_priorities(outputs)\n",
        "\n",
        "    # Write to logs.\n",
        "    self._logger.write(result)\n",
        "\n",
        "  def get_variables(self):\n",
        "    \"\"\"Network variables after a number of SGD steps.\"\"\"\n",
        "    return self._state.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAMMsi6HFlGW",
        "cellView": "form"
      },
      "source": [
        "# @title **[Solution]** DQN Learner  { form-width: \"30%\" }\n",
        "\n",
        "TrainingState =  namedtuple('TrainingState', 'params, target_params, opt_state, step')\n",
        "LearnerOutputs =  namedtuple('LearnerOutputs', 'keys, priorities')\n",
        "\n",
        "class DQNLearner(acme.Learner):\n",
        "  \"\"\"DQN learner.\"\"\"\n",
        "\n",
        "  _state: TrainingState\n",
        "\n",
        "  def __init__(self,\n",
        "               network,\n",
        "               obs_spec,\n",
        "               discount,\n",
        "               importance_sampling_exponent,\n",
        "               target_update_period,\n",
        "               data_iterator,\n",
        "               optimizer,\n",
        "               rng,\n",
        "               replay_client,\n",
        "               max_abs_reward=1.,\n",
        "               huber_loss_parameter=1.,\n",
        "               ):\n",
        "    \"\"\"Initializes the learner.\"\"\"\n",
        "\n",
        "    # Transform network into a pure function.\n",
        "    network = hk.transform(network)\n",
        "\n",
        "    def loss(params: hk.Params, target_params: hk.Params,\n",
        "             sample: reverb.ReplaySample):\n",
        "      o_tm1, a_tm1, r_t, d_t, o_t = sample.data\n",
        "      keys, probs = sample.info[:2]\n",
        "\n",
        "      # Forward pass.\n",
        "      q_tm1 = network.apply(params, o_tm1)\n",
        "      q_t_value = network.apply(target_params, o_t)\n",
        "      q_t_selector = network.apply(params, o_t)\n",
        "\n",
        "      # Cast and clip rewards.\n",
        "      d_t = (d_t * discount).astype(jnp.float32)\n",
        "      r_t = jnp.clip(r_t, -max_abs_reward, max_abs_reward).astype(jnp.float32)\n",
        "\n",
        "      # Compute double Q-learning n-step TD-error.\n",
        "      batch_error = jax.vmap(rlax.double_q_learning)\n",
        "      td_error = batch_error(q_tm1, a_tm1, r_t, d_t, q_t_value, q_t_selector)\n",
        "      batch_loss = rlax.huber_loss(td_error, huber_loss_parameter)\n",
        "\n",
        "      # Importance weighting.\n",
        "      importance_weights = (1. / probs).astype(jnp.float32)\n",
        "      importance_weights **= importance_sampling_exponent\n",
        "      importance_weights /= jnp.max(importance_weights)\n",
        "\n",
        "      # Reweight.\n",
        "      mean_loss = jnp.mean(importance_weights * batch_loss)  # []\n",
        "\n",
        "      priorities = jnp.abs(td_error).astype(jnp.float64)\n",
        "\n",
        "      return mean_loss, (keys, priorities)\n",
        "\n",
        "    def sgd_step(state, samples):\n",
        "      # Compute gradients on the given loss function and update the network\n",
        "      # using the optimizer provided at init time.\n",
        "      grad_fn = jax.grad(loss, has_aux=True)\n",
        "      gradients, (keys, priorities) = grad_fn(state.params, state.target_params,\n",
        "                                              samples)\n",
        "      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
        "      new_params = optix.apply_updates(state.params, updates)\n",
        "\n",
        "      # Update the internal state for the learner with (1) network parameters,\n",
        "      # (2) parameters of the target network, (3) the state of the optimizer,\n",
        "      # (4) Numbers of SGD steps performed by the agent.  \n",
        "      new_state = TrainingState(\n",
        "          params=new_params,\n",
        "          target_params=state.target_params,\n",
        "          opt_state=new_opt_state,\n",
        "          step=state.step + 1)\n",
        "\n",
        "      outputs = LearnerOutputs(keys=keys, priorities=priorities)\n",
        "\n",
        "      return new_state, outputs\n",
        "\n",
        "    # Internalise agent components (replay buffer, networks, optimizer).\n",
        "    self._replay_client = replay_client\n",
        "    self._iterator = data_iterator\n",
        "\n",
        "    # Since sampling is base on a priority experience replay, we need to pass\n",
        "    # the absolute td-loss values to the replay client to update priorities\n",
        "    # accordingly.\n",
        "    def update_priorities(outputs: LearnerOutputs):\n",
        "      for key, priority in zip(outputs.keys, outputs.priorities):\n",
        "        self._replay_client.mutate_priorities(\n",
        "            table='priority_table', \n",
        "            updates={key: priority})\n",
        "        \n",
        "    self._update_priorities = update_priorities\n",
        "\n",
        "    # Internalise the hyperparameters.\n",
        "    self._target_update_period = target_update_period\n",
        "\n",
        "    # Internalise logging/counting objects.\n",
        "    self._counter = counting.Counter()\n",
        "    self._logger = loggers.TerminalLogger('learner', time_delta=1.)\n",
        "\n",
        "    # Initialise parameters and optimiser state.\n",
        "    def initialization_fn(values):\n",
        "      values = tree_util.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), values)\n",
        "      # Add batch dim.\n",
        "      return tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), values)\n",
        "\n",
        "    initial_params = network.init(next(rng), initialization_fn(obs_spec))\n",
        "    initial_target_params = network.init(next(rng), initialization_fn(obs_spec))\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "\n",
        "    self._state = TrainingState(\n",
        "        params=initial_params,\n",
        "        target_params=initial_target_params,\n",
        "        opt_state=initial_opt_state,\n",
        "        step=0)\n",
        "\n",
        "    self._forward = jax.jit(network.apply)\n",
        "    self._sgd_step = jax.jit(sgd_step)\n",
        "    \n",
        "  def step(self):\n",
        "    samples = next(self._iterator)\n",
        "    # Do a batch of SGD.\n",
        "    self._state, outputs = self._sgd_step(self._state, samples)\n",
        "\n",
        "    # Update our counts and record it.\n",
        "    result = self._counter.increment(steps=1)\n",
        "\n",
        "    # Periodically update target network parameters.\n",
        "    if self._state.step % self._target_update_period == 0:\n",
        "      self._state = self._state._replace(target_params=self._state.params)\n",
        "\n",
        "    # Update priorities in replay.\n",
        "    self._update_priorities(outputs)\n",
        "\n",
        "    # Write to logs.\n",
        "    self._logger.write(result)\n",
        "\n",
        "  def get_variables(self):\n",
        "    \"\"\"Network variables after a number of SGD steps.\"\"\"\n",
        "    return self._state.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywObWtqgaSXx",
        "cellView": "form"
      },
      "source": [
        "# @title **[Only for reference]** DQN Agent implementation { form-width: \"30%\" }\n",
        "class DQN(acme.Actor):\n",
        "  def __init__(\n",
        "    self,\n",
        "    environment_spec,\n",
        "    network,\n",
        "    batch_size=256,\n",
        "    prefetch_size=4,\n",
        "    target_update_period=100,\n",
        "    samples_per_insert=32.0,\n",
        "    min_replay_size=1000,\n",
        "    max_replay_size=1000000,\n",
        "    importance_sampling_exponent=0.2,\n",
        "    priority_exponent=0.6,\n",
        "    n_step=5,\n",
        "    epsilon=0.,\n",
        "    learning_rate=1e-3,\n",
        "    discount=0.99,\n",
        "  ):\n",
        "    # Create a replay server to add data to. This is initialized as a\n",
        "    # table, and a Learner (defined separately) will be in charge of updating\n",
        "    # sample priorities based on the corresponding learner loss. \n",
        "    replay_table = reverb.Table(\n",
        "        name='priority_table',\n",
        "        sampler=reverb.selectors.Prioritized(priority_exponent),\n",
        "        remover=reverb.selectors.Fifo(),\n",
        "        max_size=max_replay_size,\n",
        "        rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "    self._server = reverb.Server([replay_table], port=None)\n",
        "    address = f'localhost:{self._server.port}'\n",
        "\n",
        "    # Use ACME reverb adder as a tool to add transition data into the replay\n",
        "    # buffer defined above.\n",
        "    self._adder = adders.NStepTransitionAdder(\n",
        "        client=reverb.Client(address),\n",
        "        n_step=n_step,\n",
        "        discount=discount)\n",
        "\n",
        "    # ACME datasets provides an interface to easily sample from a replay server.\n",
        "    dataset = datasets.make_reverb_dataset(\n",
        "        client=reverb.TFClient(address),\n",
        "        environment_spec=environment_spec,\n",
        "        batch_size=batch_size,\n",
        "        prefetch_size=prefetch_size,\n",
        "        transition_adder=True)\n",
        "    data_iterator = dataset.as_numpy_iterator()\n",
        "\n",
        "    # Create a learner that updates the parameters (and initializes them).\n",
        "    self._learner = DQNLearner(\n",
        "        network=network,\n",
        "        obs_spec=environment_spec.observations,\n",
        "        rng=hk.PRNGSequence(1),\n",
        "        optimizer=optix.adam(learning_rate),\n",
        "        discount=discount,\n",
        "        importance_sampling_exponent=importance_sampling_exponent,\n",
        "        target_update_period=target_update_period,\n",
        "        data_iterator=data_iterator,\n",
        "        replay_client=reverb.Client(address),\n",
        "    )\n",
        "    \n",
        "    # Create a feed forward actor that obtains its variables from the DQNLearner\n",
        "    # above.\n",
        "    def policy(params, key, observation):\n",
        "      action_values = hk.transform(network).apply(params, observation)\n",
        "      return rlax.epsilon_greedy(epsilon).sample(key, action_values)\n",
        "\n",
        "    self._policy = policy\n",
        "    self._rng = hk.PRNGSequence(1)\n",
        " \n",
        "    # We'll ignore the first min_observations when determining whether to take\n",
        "    # a step and we'll do so by making sure num_observations >= 0.\n",
        "    self._num_observations = -max(batch_size, min_replay_size)\n",
        "\n",
        "    observations_per_step = float(batch_size) / samples_per_insert\n",
        "    if observations_per_step >= 1.0:\n",
        "      self._observations_per_update = int(observations_per_step)\n",
        "      self._learning_steps_per_update = 1\n",
        "    else:\n",
        "      self._observations_per_update = 1\n",
        "      self._learning_steps_per_update = int(1.0 / observations_per_step)\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    observation = tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), \n",
        "                                     observation)\n",
        "    \n",
        "    key = next(self._rng)\n",
        "    params = self._learner.get_variables()\n",
        "    action = self._policy(params, key, observation)\n",
        "    action = tree_util.tree_map(lambda x: np.array(x).squeeze(axis=0), action)\n",
        "    return action \n",
        "\n",
        "  def observe_first(self, timestep):\n",
        "    self._adder.add_first(timestep)\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    self._num_observations += 1\n",
        "    self._adder.add(action, next_timestep)\n",
        "\n",
        "  def update(self):\n",
        "    # Only allow updates after some minimum number of observations have been and\n",
        "    # then at some period given by observations_per_update.\n",
        "    if (self._num_observations >= 0 and\n",
        "        self._num_observations % self._observations_per_update == 0):\n",
        "      self._num_observations = 0\n",
        "\n",
        "      # Run a number of learner steps (usually gradient steps).\n",
        "      for _ in range(self._learning_steps_per_update):\n",
        "        self._learner.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9bqngbzFu-f",
        "cellView": "form"
      },
      "source": [
        "# @title **[Running]** Training loop  { form-width: \"30%\" }\n",
        "\n",
        "# Run a `num_episodes` training episodes.\n",
        "# Rerun this cell until the agent has learned the given task.\n",
        "\n",
        "grid = build_gridworld_task(\n",
        "    task='simple', \n",
        "    observation_type=ObservationType.GRID, \n",
        "    max_episode_length=100,\n",
        ")\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "agent = DQN(\n",
        "    environment_spec=environment_spec,\n",
        "    network=network,\n",
        "    batch_size=16,\n",
        "    samples_per_insert=2,\n",
        "    epsilon=0.1,\n",
        "    min_replay_size=100)\n",
        "\n",
        "returns = run_loop(environment=environment, agent=agent, num_episodes=200, \n",
        "         logger_time_delta=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}